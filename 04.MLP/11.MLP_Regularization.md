# 11. Regularization 적용해 보기
* [딥러닝 용어 정리, L1 Regularization, L2 Regularization 의 이해, 용도와 차이 설명](https://light-tree.tistory.com/125)
---
![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile4.uf.tistory.com%2Fimage%2F99BED3425CE4B1341814F6)

## Hyper Parameter 설정


```
batch_size = 10
num_classes = 3
epochs = 200
```

## 라이브러리 및 함수설정


```
import numpy as np
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from keras import optimizers

import matplotlib.pyplot as plt
def hist_view(hist):
  print('## training loss and acc ##')
  fig, loss_ax = plt.subplots()
  acc_ax = loss_ax.twinx()

  loss_ax.plot(hist.history['loss'], 'y', label='train loss')
  loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')

  loss_ax.set_xlabel('epoch')
  loss_ax.set_ylabel('loss')
  loss_ax.legend(loc='center')

  acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')
  acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')
  acc_ax.set_ylabel('accuracy')
  acc_ax.legend(loc='center right')

  plt.show()
```

## 데이터 로드


```
# from sklearn import datasets
# iris = datasets.load_iris()
# x=iris.data
import pandas as pd
![ ! -f  iris0.csv ]&&wget http://j.finfra.com/_file/iris0.csv
iris=pd.read_csv("iris0.csv")


```

## input data 와 target data 설정


```
x=iris.iloc[:,0:4].values
y_text=iris.iloc[:,4:5]

sets=y_text.drop_duplicates()["Species"].tolist()
encoder={k:v for v,k in enumerate(sets)}
y_num=[ encoder[i] for i in y_text["Species"].tolist() ]



```

## 훈련 데이터와 평가 데이터 분리


```
# iris.target → y
y= keras.utils.to_categorical(y_num, num_classes)

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=0)
x_train.shape,x_test.shape,y_train.shape,y_test.shape

```

## Model 에 Regularization 적용


```
from keras.layers import BatchNormalization, Dropout
from keras import regularizers

l2=regularizers.l2(l2=0.01)

model = Sequential()

model.add(Dense(6, activation='relu', input_shape=(4,)))
model.add(Dense(4, activation='relu',kernel_regularizer=l2))

model.add(Dense(4, activation='relu',kernel_regularizer='l1'))

model.add(Dense(num_classes, activation='softmax'))
model.summary()

adam=optimizers.adam_v2.Adam(0.001)
model.compile(loss='categorical_crossentropy',
              optimizer=adam,
              metrics=['accuracy'])

# batch_size,epochs
spe=int(x_train.shape[0]/batch_size )
hist=model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          steps_per_epoch=spe,
          validation_data=(x_test, y_test))

# help(model.fit)

```

## 결과 확인


```
hist_view(hist)
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

```


    
![png](11.MLP_Regularization_files/11.MLP_Regularization_14_0.png)
    


    Test loss: 0.1976756602525711
    Test accuracy: 0.9777777791023254



```
model.layers[2].weights
```


```
decoder = {k:v for k,v in enumerate( sets )}
decoder
```


```
r=np.argmax(model.predict(x_test[:10,:]), axis=-1)
[decoder[i] for i in r]
```

# 시각화도 하고 만든 모델도 저장하고 만든 모델에 대한 평가도 하고, 하고 싶은 것이 많지만, 다음 장에서...==^^==


```

```
