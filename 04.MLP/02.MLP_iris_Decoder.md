# 2. Decoder 를 적용

## simple Model 만들기
[1. MLP_verySimple.ipynb 참고](https://github.com/Finfra/TensorflowStudyExample/blob/master/s2.5/3.KerasIntro/1.MLP_verySimple.ipynb)



```
import numpy as np
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
```


```
from sklearn import datasets
iris = datasets.load_iris()
x=iris.data
y = keras.utils.to_categorical(iris.target, 3)

```


```
model = Sequential()
model.add(Dense(4, activation='relu', input_shape=(4,)))
model.add(Dense(3, activation='softmax'))
model.summary()

```


```
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

```


```
model.fit(x, y,
          batch_size=10,
          epochs=100,
          verbose=0
)

score = model.evaluate(x, y, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

```

## Decoder 만들기
0,1,2 형태가 아닌 우리가 알고 싶은 실제 이름을 가져와서 보여줄 수 있도록 하는 Decoding 함수를 만들기


```
decoder = {k:v for k,v in enumerate( iris.target_names )}
```


```
x_test=np.array( [ [5.1, 3.5, 1.4, 0.2],[5. , 2. , 3.5, 1. ]])
r=np.argmax(model.predict(x_test), axis=-1)
[decoder[i] for i in r]
```

# Train데이터와 Test데이터가 분리 되어 있지 않아서 Overfitting을 알 수 없음. 
