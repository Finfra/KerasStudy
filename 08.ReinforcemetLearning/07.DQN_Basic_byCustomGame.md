# Custom DQN basic

## Import Library


```python
import collections
# import gym
import numpy as np
import tensorflow as tf
import tqdm

from matplotlib import pyplot as plt
from tensorflow.keras import layers
from tensorflow.keras import Sequential
from typing import Any, List, Sequence, Tuple
```

## Make Custom Game
규칙. 
- 4 개의 구멍이 있다.<br>
- 구멍의 번호를 선택하면 그 구멍에 2개의 공이 들어간다.<br>
- 선택한 구멍의 양 옆의 구멍의 공의 갯수는 1/2 이 된다.<br>
- 한 구멍의 공의 갯수가 4개를 넘어가면 게임이 종료된다.


```python
class virtual_game:
  pan = None        # 게임판
  hole_numbers = 0  # 구멍의 갯수
  hole_depth = 0    # 구멍의 깊이
  score = 0         # 스코어 계산에 사용하는 변수
  count = 0         # 해당 행동의 reward 를 반환하기 위한 함수
  done = False      # 게임의 종료 여부 판단
  path = None       # 게임 종료 후 Replay 를 위해 행동 저장

  def __init__( self, hole_numbers=4, hole_depth=4):
    self.hole_numbers = hole_numbers
    self.hole_depth = hole_depth
    self.reset()

  # 게임 초기화 함수
  def reset(self):
    self.pan = np.zeros(self.hole_numbers)
    self.path = []
    self.score = 0
    self.count = 0
    self.done = False
    return self.pan
  
  # 현재 스코어 반환 함수
  def scored(self):
    self.score = 0
    for i in self.pan:
      self.score = self.score + i
    return self.score

  # hole 의 번호를 입력받아 게임을 실행하는 함수
  # 게임판 정보와 count(reward)와 게임종료 여부를 반환
  def play(self, hole):
    #reward 초기화
    self.count = 0
    # hole 이 잘못 들어온 경우 변화없음.
    if (hole > self.hole_depth-1) or (hole < 0) :
      return self.pan , self.count, self.done, 
    
    # hole 이 제대로 들어왔을 시 게임 진행
    # replay 를 위한 path 저장
    self.path.append(hole)
    # hole 의 값 증가 시키기
    # reward 도 증가
    self.count = self.count +4
    self.pan[hole] = self.pan[hole] +2

    # hole 이 4보다 커진 경우 4로 고정시켜주고 게임종료
    if self.pan[hole] > 4 :
      # 게임이 종료되는 것 자체가 좋지 않다고 생각하여 패널티 부여
      # self.count = self.count -5
      self.pan[hole] = 4
      self.done = True
      # 만약에 판 전체의 score 가 10을 넘어갈 경우 reward 100 으로 고정
      if self.scored() > 10 :
        self.count = 100
      return self.pan , self.count, self.done, 
    # hole 양쪽의 공의 갯수 1/2 로 줄이기
    if (hole-1) >= 0 : # 왼쪽 홀이 있을 시
        self.count = self.count -1
        self.pan[hole-1] = round(self.pan[hole-1]/2 + 0.1)
    if (hole+1) <= 3 : # 오른쪽 홀이 있을 시
        self.count = self.count -1
        self.pan[hole+1] = round(self.pan[hole+1]/2 +0.1)

    return self.pan , self.count, self.done, 
```

### Game test


```python
env = virtual_game()
```


```python
env.reset()
actions = [0,2,1,3,0,0,0]
for i in actions :
  env.play(i)
  print('action = ',i)
  print(env.pan)
  if env.done :
    break;
print('================')
print(env.scored())
print(env.pan)
```

## Model 설계


```python
env.pan.shape
```


```python
num_actions = env.hole_numbers
```


```python
model = Sequential()
model.add(layers.InputLayer(batch_input_shape=(num_actions,1)))
model.add(layers.Dense(10, activation='sigmoid'))
model.add(layers.Dense(15, activation='sigmoid'))
model.add(layers.Dense(num_actions, activation='linear'))
model.compile(loss='mse', optimizer='adam', metrics=['mae'])
```

## Model 훈련


```python
num_episodes = 1000
```


```python
# now execute the q learning
y = 0.95              # y
eps = 0.5             # epsilon
decay_factor = 0.999  # decay factor
r_avg_list = []       # reward 기록
for i in range(num_episodes):
    s = env.reset()   # reset game
    new_s = s
    eps *= decay_factor
    if i % 100 == 0:    # 100 번 훈련할 때마다 출력
        print("Episode {} of {}".format(i + 1, num_episodes))
    done = False
    r_sum = 0
    while not done:
        # 훈련 시작
        if np.random.random() < eps:
          # 랜덤으로 액션 
          a = np.random.randint(0, num_actions)
        else:
          # 모델 예측으로 액션
          a = np.argmax(model.predict(new_s))

        # 액션 값으로 게임 진행
        new_s, r, done = env.play(a)

        # target 설정
        target = r + y * np.argmax(model.predict(new_s))
        target_vec = model.predict(s)
        # 예측으로 나온 action 값이 홀의 갯수를 넘어갈 경우 넘어가지 않게 해줌.
        if a >= num_actions :
          a = a % num_actions
          
        target_vec[a] = target
        hist = model.fit(np.array(s), target_vec.reshape(num_actions,-1), epochs=1, verbose=0)
        s = new_s
        r_sum +=r
    print (s)
    print( r_sum)
    r_avg_list.append(r_sum / 1000)
    if r_sum > 90 :
      break

    
```


```python
from matplotlib import pyplot
pyplot.plot(r_avg_list)
pyplot.legend()
pyplot.show()
```


    
![png](07.DQN_Basic_byCustomGame_files/07.DQN_Basic_byCustomGame_15_0.png)
    



```python
print(r_avg_list)
```


```python
bestpath = env.path
print(bestpath)
```


```python
env.reset()
for i in bestpath :
  env.play(i)
  print('action = ',i)
  print(env.pan)
  if env.done :
    break;
print('================')
print(env.scored())
print(env.pan)
```


```python

```
